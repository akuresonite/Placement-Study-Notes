# Machine Learning

<!-- # [1] Regression Decision Trees -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[1] üê±üê∂üöÄ Regression Decision Trees üî•!</h1>
</div>

## Algorithm

1. **Initialize the Root Node**:
   - Start with the entire dataset at the root.

2. **Split Criterion**:
   - For each feature, identify possible split points.
   - For each split point:
     - **Divide** the data into two subsets, `left` and `right`, based on the split.
     - **Calculate the Mean** of the target variable for each subset (`mean_left` and `mean_right`).
     - **Compute the MSE** for each subset:
       - For each subset, compute the squared difference between each target value and the subset mean, sum these squared differences, and divide by the number of samples in the subset.
     - **Calculate Weighted MSE** for the split by summing the MSEs of the left and right subsets, weighted by the size of each subset.
   - **Select the Best Split**:
     - Choose the feature and split point that give the lowest weighted MSE.

3. **Split the Data**:
   - Divide the dataset into two subsets based on the best split.

4. **Recursive Splitting**:
   - Repeat steps 2-3 recursively for each subset until:
     - Maximum depth is reached,
     - MSE is below a predefined threshold, or
     - Minimum samples per node is reached.

5. **Leaf Nodes**:
   - For each leaf node, calculate the mean value of the target variable as the prediction for that node.

6. **Prediction**:
   - To predict for a new data point, traverse the tree based on its features, reaching a leaf node where the prediction is the stored mean value.


<!-- # [2] Cost Complexity Pruning in Regression Decision Trees -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[2] üê±üê∂üöÄ Cost Complexity Pruning in Regression Decision Trees üî•!</h1>
</div>

- Cost Complexity Pruning is a method to simplify a regression decision tree by removing branches that provide little predictive power, aiming to reduce overfitting and enhance model generalization.

  1. **Initialize Parameters**:
     - Set $\alpha$ to control the balance between tree complexity and fit.
     - Fully grow the regression tree without any pruning.

  2. **Calculate Cost Complexity for Each Subtree**:
     - For each non-leaf node in the fully grown tree, calculate the **Cost Complexity**:
       $$
       \text{Cost} = \text{MSE of the subtree} + \alpha \times \text{number of leaf nodes in the subtree}
       $$
     - Store the cost complexity of each subtree for potential pruning.

  3. **Identify Candidate Nodes for Pruning**:
     - Sort nodes by their cost complexity.
     - Select nodes that contribute the least to reducing MSE relative to their complexity.

  4. **Iterative Pruning**:
     - **Prune** the subtree with the lowest cost complexity among candidates.
     - Recalculate the cost complexity for the resulting pruned tree.
     - Repeat pruning until removing any further nodes would increase the overall cost complexity.

  5. **Determine Optimal $\alpha$ Using Cross-Validation**:
     - Use cross-validation to evaluate different values of $\alpha$.
     - For each value of $\alpha$, generate a pruned version of the tree.
     - Select the $\alpha$ that minimizes validation error, which balances between underfitting and overfitting.

  6. **Select Final Pruned Tree**:
     - Choose the pruned tree corresponding to the optimal $\alpha$ from the cross-validation step as the final model.

- This pruning process yields a regression tree that balances predictive accuracy with model simplicity, improving generalization on unseen data.

- The best $\alpha$ in Cost Complexity Pruning is chosen through **cross-validation**. Here‚Äôs how the process works:

  1. **Generate Candidate Trees for Each $\alpha$**:
     - Grow a fully expanded tree, then prune it iteratively using various $\alpha$ values.
     - For each $\alpha$, prune the tree to obtain a simpler model based on the cost complexity criterion.

  2. **Cross-Validation**:
     - Split the training data into $k$ folds.
     - For each $\alpha$ value:
       - Perform cross-validation by training on $(k-1)$ folds and evaluating on the remaining fold.
       - Compute the mean squared error (MSE) across the validation folds for each pruned tree.

  3. **Select Optimal $\alpha$**:
     - Choose the $\alpha$ that yields the **lowest average validation error** (MSE) across the $k$ folds.
     - This $\alpha$ represents the best balance between model complexity and prediction accuracy, minimizing overfitting.

  4. **Final Model**:
     - Using the optimal $\alpha$, prune the fully grown tree to produce the final model.

- This approach ensures that the chosen $\alpha$ leads to a pruned tree with the best generalization performance.

<!-- # [3] Classification Decision Trees -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[3] üê±üê∂üöÄ Classification Decision Trees üî•!</h1>
</div>

- Leaves contains mixture of ***'yes'*** and ***'no'***, are called Impure.
- There are the three ways to quantifying entropy of a leaf:
  1. **Entropy**:
     - Entropy measures the impurity of a node based on the probability of each class:
       $$
       \text{Entropy}(S) = - \sum_{i=1}^{k} p_i \log_2(p_i)
       $$
     - Where $p_i$ is the probability of class $i$ in dataset $S$, and $k$ is the total number of classes.

  2. **Gini Impurity** (often referred to as Gini Entropy):
     - Gini impurity calculates the probability of misclassifying a randomly chosen element:
       $$
       \text{Gini}(S) = 1 - \sum_{i=1}^{k} p_i^2
       $$
     - Where $p_i$ is the probability of class $i$ in dataset $S$.

  3. **Information Gain**:
     - Information Gain (IG) measures the reduction in entropy after a dataset is split on a feature:
       $$
       \text{Information Gain}(S, A) = \text{Entropy}(S) - \sum_{v \in \text{values}(A)} \frac{|S_v|}{|S|} \cdot \text{Entropy}(S_v)
       $$
     - Here:
       - $S$ is the original dataset,
       - $A$ is the attribute (feature) being split,
       - $S_v$ is the subset of $S$ where attribute $A$ has value $v$,
       - $\text{values}(A)$ represents all possible values of attribute $A$,
       - $|S|$ and $|S_v|$ are the sizes of $S$ and $S_v$, respectively.

- Total Gini Impurity of node = weighted average of Gini Impurities for the Leaves.
- Using Gini Impurity instead of Information Gain is common in certain implementations, like the CART (Classification and Regression Tree) algorithm, where Gini Impurity is preferred for its simplicity and computational efficiency.

- ## Algorithm

  1. **Initialize Root Node**:
     - Start with the entire dataset at the root.

  2. **Calculate Impurity for Each Feature**:
     - For each feature, calculate impurity measures (Entropy or Gini Impurity) for possible splits.
     - For each split:
       - **Divide** the data into two subsets based on the split.
       - **Calculate the Weighted Impurity** for each subset, based on the chosen impurity measure (Entropy or Gini).

  3. **Select Best Split**:
     - Choose the feature and split point that give the highest Information Gain (i.e., maximum reduction in impurity), or
     - Choose the feature and split point with the lowest Gini Impurity. This corresponds to the split that results in the most "pure" (least mixed) subsets, effectively reducing misclassification probability.

  4. **Split the Node**:
     - Create child nodes based on the best split.
     - Assign each subset to the corresponding child node.

  5. **Recursive Splitting**:
     - For each child node, repeat steps 2‚Äì4 recursively until a stopping criterion is met:
       - Maximum depth reached,
       - Minimum samples per node,
       - No further reduction in impurity.

  6. **Assign Class Labels at Leaf Nodes**:
     - For each leaf node, assign a class label based on the majority class in that node.

  7. **Prediction**:
     - To classify a new sample, start from the root and traverse the tree based on the sample‚Äôs feature values, following the appropriate branches until reaching a leaf node. Return the class label at that leaf.

- Because so few people in Leaf, it's hard to have confidence that it will do a great job making predictions with future data.
- And it's possible that we have Overfit the data.
- Regardless, in practice, there are two main ways to deal with this problem.
  1. One method is called Pruning.
  2. Alternatively, we can put limits on how trees grow, for example, by requiring 3 or more people per leaf (cross-validation).
  3. Now we end up with an Impure Leaf.
  4. But also a better sense of the accuracy of our prediction.

<!-- # [4] Random Forests -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[4] üê±üê∂üöÄ Random Forests üî•!</h1>
</div>

Random Forest combines multiple decision trees to create a strong, robust model by averaging out the predictions, reducing overfitting, and improving generalization on unseen data.

1. **Initialize Parameters**:
   - Set the number of trees, n_trees, and other hyperparameters (e.g., max depth, min samples per split).

2. **Bootstrap Sampling**:
   - For each tree, draw a **bootstrap sample** from the training dataset (randomly sample with replacement). This ensures each tree has a slightly different dataset.

3. **Build Decision Trees**:
   - For each tree:
     1. **Select Random Features**:
        - For each split in the tree, randomly select a subset of features instead of considering all features.
     2. **Construct Tree Using Selected Features**:
        - Build the tree using the bootstrap sample by applying the Classification Decision Tree or Regression Decision Tree algorithm, with the selected subset of features at each split.
        - Grow the tree until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf).

4. **Aggregate Predictions**:
   - For classification, use **majority voting** across all trees: each tree casts a vote for the predicted class, and the class with the most votes is the final prediction.
   - For regression, use **average prediction**: take the mean of predictions from all trees.

5. **Prediction for New Data**:
   - To make predictions for new samples, pass each sample through all trees in the forest and aggregate the predictions (using majority voting for classification or averaging for regression).

- Bootstrapping the data plus using the aggregate to make a decision is called `Bagging"`

- The **Out-of-Bag (OOB) score** is a validation metric used in Random Forests to estimate the model's accuracy without needing a separate validation set. It leverages the unique sampling method of Random Forests (bootstrap sampling) to provide an internal measure of model performance.

    1. **Bootstrap Sampling**:
       - When each decision tree in the Random Forest is built, it is trained on a bootstrap sample, which is a random subset of the original dataset with replacement.
       - As a result, about **one-third of the data** is left out of the bootstrap sample for each tree; this is known as the **out-of-bag (OOB) data** for that tree.

    2. **OOB Prediction**:
       - For each sample left out of the training data for a particular tree, the tree can predict the sample's outcome since it wasn‚Äôt included in the training of that tree.
       - Each sample is predicted by only the trees for which it was OOB, providing an unbiased estimate of the model‚Äôs performance on that sample.

    3. **Calculating the OOB Score**:
       - The OOB score is computed by aggregating predictions for each sample from its corresponding OOB trees and comparing them to the actual labels.
       - For classification, the OOB score is typically calculated as the accuracy of predictions.
       - For regression, the OOB score is often calculated as the mean squared error or R-squared.

- Advantages of OOB Score:
  - **No Need for Separate Validation Set**: It provides a built-in validation estimate, preserving more data for training.
  - **Unbiased Performance Metric**: The OOB score is a good estimator of the generalization accuracy of the Random Forest model.
