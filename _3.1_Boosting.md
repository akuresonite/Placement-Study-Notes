# Machine Learning

<!-- # [1] AdaBoost -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[1] ğŸ±ğŸ¶ğŸš€ Adapative Boosting - AdaBoost ğŸ”¥!</h1>
</div>

NOTE: Here i will shows how to combine AdaBoost with Decision Trees, because that is the most common way to use AdaBoost.

So let's start by using Decision Trees and Random Forests to explain the three main concepts behind AdaBoost!

1. **Use of Stumps**
    - In a Random Forest, each time you make a tree, you make a full sized tree.
    - Some trees might be bigger than others, but there is no predetermined maximum depth.
    - In contrast, in a Forest of Trees made with AdaBoost, the trees are usually just a node and two leaves.
    - A tree with just one node and two leaves is called a stump.
    - So this is really a Forest of Stumps rather than trees.
    - Stumps are not great at making accurate classifications.
    - A full sized Decision Tree would take advantage of all features to make a decision.
    - But a Stump can only use one variable to make a decision.
    - Thus, Stumps are technically "weak learners".
   > AdaBoost combines a lot of "weak learners" to make classifications. The weak learners are almost aways stumps.
2. **Say of each Stump**
    - In a Random Forest, each tree has an equal vote on the final classification.
    - In contrast, in a Forest of Stumps made with AdaBoost, some stumps get more say in the final classification than others.
   > Some stumps get more say in the classification than others.
3. **Influence of Previous Stump**
   - Lastly, in a Random Forest, each decision tree is made independently of the others.
   - In other words, it doesn't matter if this tree was made first.
   - In contrast, in a Forest of Stumps made with AdaBoost, order is important.
   - The errors that the first stump makes influence how the second stump is made and the errors that the second stump makes influence how the third stump is made etc. etc. etc..
   > Each stump is made by taking the previous stump's mistakes into account.
