# Machine Learning

<!-- # [1] AdaBoost -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[1] üê±üê∂üöÄ Adapative Boosting - AdaBoost üî•!</h1>
</div>

NOTE: Here i will shows how to combine AdaBoost with Decision Trees, because that is the most common way to use AdaBoost.

So let's start by using Decision Trees and Random Forests to explain the three main concepts behind AdaBoost!

1. **Use of Stumps**
    - In a Random Forest, each time you make a tree, you make a full sized tree.
    - Some trees might be bigger than others, but there is no predetermined maximum depth.
    - In contrast, in a Forest of Trees made with AdaBoost, the trees are usually just a node and two leaves.
    - A tree with just one node and two leaves is called a stump.
    - So this is really a Forest of Stumps rather than trees.
    - Stumps are not great at making accurate classifications.
    - A full sized Decision Tree would take advantage of all features to make a decision.
    - But a Stump can only use one variable to make a decision.
    - Thus, Stumps are technically "weak learners".
   > AdaBoost combines a lot of "weak learners" to make classifications. The weak learners are almost aways stumps.
2. **Say of each Stump**
    - In a Random Forest, each tree has an equal vote on the final classification.
    - In contrast, in a Forest of Stumps made with AdaBoost, some stumps get more say in the final classification than others.
   > Some stumps get more say in the classification than others.
3. **Influence of Previous Stump**
   - Lastly, in a Random Forest, each decision tree is made independently of the others.
   - In other words, it doesn't matter if this tree was made first.
   - In contrast, in a Forest of Stumps made with AdaBoost, order is important.
   - The errors that the first stump makes influence how the second stump is made and the errors that the second stump makes influence how the third stump is made etc. etc. etc..
   > Each stump is made by taking the previous stump's mistakes into account.

## Algorithm

Of **AdaBoost** for both regression and classification:

### **Algorithm for AdaBoost (General Framework)**

1. **Initialize Weights**:
   - Assign equal weights to all training samples:
   - $w_i = \frac{1}{N}, \, \forall i = 1, 2, \dots, N$
   - where $N$ is the total number of samples.

2. **Repeat for Each Weak Learner** ($m = 1$ to $M$):
   - Train a weak learner $h_m(x)$ using the weighted training data.

3. **Compute Error**:
   - Calculate the weighted error for the weak learner:
   - $\epsilon_m = \frac{\sum_{i=1}^{N} w_i \cdot \mathbb{I}(h_m(x_i) \neq y_i)}{\sum_{i=1}^{N} w_i}$
   - For regression, use:
   - $\epsilon_m = \frac{\sum_{i=1}^{N} w_i \cdot |h_m(x_i) - y_i|}{\sum_{i=1}^{N} w_i}$

4. **Calculate Alpha** (Weight of the Weak Learner):
   - For classification:
   - $\alpha_m = \frac{1}{2} \ln\left(\frac{1 - \epsilon_m}{\epsilon_m}\right)$
   - For regression:
   - $\alpha_m = \text{minimize a loss function (e.g., squared loss or absolute loss)}.$

5. **Update Weights**:
   - For classification:
   - $w_i \leftarrow w_i \cdot \exp\left(-\alpha_m \cdot y_i \cdot h_m(x_i)\right)$
   - For regression:
   - $w_i \leftarrow w_i \cdot \exp\left(-\alpha_m \cdot |y_i - h_m(x_i)|\right)$
   - Normalize weights so that $\sum_{i=1}^{N} w_i = 1$.

5. **Resample the Dataset** (Optional, depending on implementation):
   - Create a new dataset by resampling from the original dataset using the updated weights as probabilities.
   - Samples with higher weights are more likely to appear multiple times in the new dataset, while samples with lower weights may appear less frequently or not at all.

6. **Train the Next Weak Learner**:
   - Train the next weak learner on this newly weighted or resampled dataset.
   - This iterative process ensures that each new weak learner focuses on the "harder" samples, improving the overall model's performance.

7. **Aggregate Weak Learners**:
   - For classification, combine predictions using a weighted vote:
   - $H(x) = \text{sign}\left(\sum_{m=1}^{M} \alpha_m \cdot h_m(x)\right)$
   - For regression, combine predictions as a weighted sum:
   - $H(x) = \sum_{m=1}^{M} \alpha_m \cdot h_m(x)$

---

### Differences for Regression vs. Classification:
1. **Error Metric**:
   - Classification: Weighted misclassification error.
   - Regression: Weighted loss (e.g., squared or absolute error).

2. **Final Prediction**:
   - Classification: Majority vote using sign of weighted sum.
   - Regression: Weighted average of predictions.

---

This framework ensures that the algorithm iteratively focuses on harder-to-predict samples by adjusting their weights and combining weak learners to build a strong model.

<!-- # [2] Gradient Boost -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[2] üê±üê∂üöÄ Gradient Boost Regressionüî•!</h1>
</div>

Let's briefly compare and contrast AdaBoost and Gradient Boost.

>**AdaBoost**
>
>1. AdaBoost starts by building a very short tree, called a Stump, from the Training Data.
>2. The amount of say that the stump has on the final output is based on how well it compensated for those previous errors.
>3. Then AdaBoost builds the next stump based on errors that the previous stump made.
>4. Then AdaBoost builds another stump based on the errors made by the previous stump.
>5. Then AdaBoost continues to make stumps in this fashion until it has made the number of stumps you asked for, or it has a perfect fit.


>**Gradient Boost**
>
>1. In contrast, Gradient Boost starts by making a single leaf, instead of a tree or stump.
>2. This leaf represents an initial guess for the Weights of all of the samples.
>3. When trying to Predict a continuous value like Weight, the first guess is the the average value.
>4. Then Gradient Boost builds a tree.
>5. Like AdaBoost, this tree is based on the errors made by the previous tree.
>6. But unlike AdaBoost, this tree is usually larger than a stump.
>7. That said, Gradient Boost still restricts the size of the tree.
>8. In practice, people often set the maximum number of leaves to be between 8 and 32.
>9. Thus, like AdaBoost, Gradient Boost builds fixed sized trees based on the previous tree's errors, but unlike AdaBoost, each tree can be larger than a stump.
>10. Also like AdaBoost, Gradient Boost scales the trees. However, Gradient Boost scales all trees by the same amount.
>11. Then Gradient Boost builds another tree based on the errors made by the previous tree and then it scales the tree.
>12. And Gradient Boost continues to build trees in this fashion until it has made the number of trees you asked for, or additional trees fail to improve the fit.


## Algorithm for Gradient Boosting Regression

1. **Initialize the Model**:
   - Start with an initial prediction for all samples, typically the mean of the target values:
     - $F_0(x) = \text{mean}(y)$
   - Compute the Pseudo residuals:
     - $r_i^{(0)} = y_i - F_0(x_i), \, \forall i = 1, \dots, N$

2. **For Each Iteration (from $m = 1$ to $M$)**:
   - **Fit a Weak Learner**:
     - Train a weak learner $h_m(x)$ (e.g., a decision tree) to predict the residuals $r_i^{(m-1)}$ from the previous iteration.

   - **Compute the Step Size**:
     - Use a learning rate $\eta$ (e.g., 0.1) to scale the contribution of the weak learner.
     - Optionally, minimize the residual sum of squares (RSS) to determine the optimal step size:
       - $\gamma_m = \arg\min_{\gamma} \sum_{i=1}^{N} \left( r_i^{(m-1)} - \gamma \cdot h_m(x_i) \right)^2$

   - **Update the Model**:
     - Add the scaled weak learner to the current model:
       - $F_m(x) = F_{m-1}(x) + \eta \cdot \gamma_m \cdot h_m(x)$

   - **Update Residuals**:
     - Compute new residuals based on the updated model:
       - $r_i^{(m)} = y_i - F_m(x_i), \, \forall i = 1, \dots, N$

3. **Final Prediction**:
   - After $M$ iterations, the final model is:
     - $F_M(x) = F_0(x) + \sum_{m=1}^{M} \eta \cdot \gamma_m \cdot h_m(x)$

---
### Sumarry:

1. We start with a leaf that is the average value of the variable we want to Predict.
2. Then we add a tree based on the Residuals, the difference between the Observed values and the Predicted values.
3. And we scale the tree's contribution to the final Prediction with a Learning Rate.
4. Then we add another tree based on the new Residuals.
5. And we keep adding trees based on the errors made by the previous tree.

### Key Features of Gradient Boosting Regression:

- **Residual Focus**: Each weak learner focuses on minimizing the residuals (errors) of the previous model.
- **Learning Rate**: Controls the contribution of each weak learner to avoid overfitting.
- **Iterative Improvement**: Combines weak learners iteratively to build a strong model.

This algorithm effectively minimizes the loss function (e.g., mean squared error) over multiple iterations to improve predictive performance.


<!-- # [3] eXtream Gradient Boost -->
<div style="border-radius: 30px 0 30px 0px; border: 2px solid #00ea98; padding: 20px; background-color: #000000; text-align: center; box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.2);">
    <h1 style="color: #87CEEB; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); font-weight: bold; margin-bottom: 10px; font-size: 36px;">[2] üê±üê∂üöÄ <img src="https://xgboost.ai/images/logo/xgboost-logo-trimmed.png" width=200/>üî•!</h1>
</div>

**Algorithm of XGBoost Regression and Classification**

---

**Introduction**

XGBoost (eXtreme Gradient Boosting) is an optimized implementation of gradient boosting decision trees designed for high performance and speed. It has become a powerful tool for both regression and classification tasks in machine learning. The core idea behind XGBoost is to build additive predictive models by sequentially fitting new models to correct the errors made by the existing ones, using gradient descent optimization in function space.

This document provides a detailed step-by-step algorithm of XGBoost for regression and classification tasks, including all mathematical expressions and explanations.

---

**General Framework**

XGBoost aims to minimize an objective function $\mathcal{L}$ that consists of a differentiable convex loss function $l$ and a regularization term $\Omega$:

$\mathcal{L} = \sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$

- $n$: Number of training examples.
- $y_i$: True label of the $i$-th example.
- $\hat{y}_i$: Predicted value for the $i$-th example.
- $f_k$: The $k$-th regression tree (decision tree).
- $K$: Number of trees.
- $\Omega(f)$: Regularization term for a tree $f$ to prevent overfitting.

Each prediction $\hat{y}_i$ is the sum of the outputs from all trees:

$\hat{y}_i = \sum_{k=1}^K f_k(x_i)$

- $x_i$: Feature vector of the $i$-th example.

---

**Algorithm Steps**

### **1. Initialization**

- **Regression**: Initialize predictions with a constant value, usually the mean of the target variable.

  $  \hat{y}_i^{(0)} = \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i
  \]

- **Classification**: Initialize with a constant probability or log-odds, depending on the loss function.

### **2. Iterative Boosting**

For $t = 1$ to $T$ (number of boosting rounds):

#### **a. Compute Gradients and Hessians**

For each training example $i$:

- **First-order derivative (Gradient)**:

  $  g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}
  \]

- **Second-order derivative (Hessian)**:

  $  h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{\partial [\hat{y}_i^{(t-1)}]^2}
  \]

**Note**: The specific form of $g_i$ and $h_i$ depends on the loss function used.

#### **b. Define the Regularized Objective Function**

The objective function to be minimized at iteration $t$ is approximated using a second-order Taylor expansion:

$\mathcal{L}^{(t)} \approx \sum_{i=1}^n \left[ g_i f_t(x_i) + \frac{1}{2} h_i [f_t(x_i)]^2 \right] + \Omega(f_t)$

- $f_t(x_i)$: Output of the new tree $f_t$ for example $x_i$.

#### **c. Define the Regularization Term**

The regularization term for tree $f_t$ is:

$\Omega(f_t) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2$

- $T$: Number of leaves in the tree.
- $w_j$: Weight of leaf $j$.
- $\gamma$: Complexity parameter for the number of leaves.
- $\lambda$: L2 regularization term on leaf weights.

#### **d. Optimize the Tree Structure**

The goal is to find the tree $f_t$ that minimizes the objective function. Since exhaustive search is impractical, a greedy algorithm is used:

1. **Start with all data at the root node.**
2. **For each split candidate (feature and threshold):**

   - Partition the data into left ($I_L$) and right ($I_R$) nodes.
   - Compute the gain from the split:

     $     \mathcal{G} = \frac{1}{2} \left( \frac{[\sum_{i \in I_L} g_i]^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{[\sum_{i \in I_R} g_i]^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{[\sum_{i \in I} g_i]^2}{\sum_{i \in I} h_i + \lambda} \right) - \gamma
     \]

   - $I$: Indices of data in the current node.

3. **Choose the split with the highest gain $\mathcal{G}$.**
4. **Prune the split if $\mathcal{G} < 0$.**
5. **Repeat recursively for child nodes until stopping criteria are met (e.g., maximum depth, minimum number of samples).**

#### **e. Calculate Optimal Leaf Weights**

For each leaf $j$:

$w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}$

- $I_j$: Indices of data in leaf $j$.

#### **f. Update the Model**

Update the predictions:

$\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i) = \hat{y}_i^{(t-1)} + w_{q(x_i)}$

- $q(x_i)$: Function mapping $x_i$ to a leaf index in tree $f_t$.
- $w_{q(x_i)}$: Weight of the leaf that $x_i$ falls into.

### **3. Final Prediction**

After $T$ iterations, the final prediction is:

$\hat{y}_i = \hat{y}_i^{(T)} = \sum_{t=1}^T f_t(x_i)$

---

**Mathematical Expressions for Loss Functions**

### **Regression Loss Functions**

#### **a. Squared Error Loss**

$l(y_i, \hat{y}_i) = \frac{1}{2} (y_i - \hat{y}_i)^2$

- **Gradient**:

  $  g_i = \hat{y}_i^{(t-1)} - y_i
  \]

- **Hessian**:

  $  h_i = 1
  \]

### **Classification Loss Functions**

#### **a. Logistic Loss (Binary Classification)**

For $y_i \in \{0, 1\}$:

$l(y_i, \hat{y}_i) = - [ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) ]$

- $p_i = \sigma(\hat{y}_i^{(t-1)}) = \frac{1}{1 + e^{-\hat{y}_i^{(t-1)}}}$

- **Gradient**:

  $  g_i = p_i - y_i
  \]

- **Hessian**:

  $  h_i = p_i (1 - p_i)
  \]

#### **b. Softmax Loss (Multi-class Classification)**

For $K$ classes and $y_i \in \{1, 2, ..., K\}$:

$l(y_i, \hat{\mathbf{y}}_i) = - \log\left( \frac{e^{\hat{y}_{i y_i}}}{\sum_{k=1}^K e^{\hat{y}_{i k}}} \right)$

- $\hat{\mathbf{y}}_i$: Vector of predicted scores for all classes.

- **Gradient** for class $k$:

  $  g_{ik} = p_{ik} - \mathbb{I}[y_i = k]
  \]

- **Hessian**:

  $  h_{ik} = p_{ik} (1 - p_{ik})
  \]

- $p_{ik} = \frac{e^{\hat{y}_{i k}}}{\sum_{k'=1}^K e^{\hat{y}_{i k'}}}$
- $\mathbb{I}[\cdot]$: Indicator function.

---

**Detailed Steps with Explanations**

### **Step 1: Data Preparation**

- **Input**: Training data $D = \{(x_i, y_i)\}_{i=1}^n$.
- **Goal**: Predict $y_i$ from $x_i$.

### **Step 2: Model Initialization**

- Set initial predictions $\hat{y}_i^{(0)}$ based on the problem type.

### **Step 3: Iterate Over Trees**

For each boosting iteration $t$:

#### **a. Compute Residuals**

- Residuals represent the negative gradients of the loss function.

#### **b. Fit a Regression Tree**

- Use the residuals $g_i$ and Hessians $h_i$ as target values.
- The tree $f_t$ maps $x_i$ to a leaf weight $w_j$.

#### **c. Split Finding**

- For each feature and split point:

  - **Calculate Gain**:

    $    \mathcal{G} = \frac{1}{2} \left( \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L + H_R + \lambda} \right) - \gamma
    \]

    - $G_L = \sum_{i \in I_L} g_i$, $H_L = \sum_{i \in I_L} h_i$
    - $G_R = \sum_{i \in I_R} g_i$, $H_R = \sum_{i \in I_R} h_i$

- **Choose Split with Maximum Gain**.

#### **d. Prune Tree**

- Stop splitting when the gain is less than zero or below a threshold.

#### **e. Assign Leaf Weights**

- For each leaf $j$:

  $  w_j^* = -\frac{G_j}{H_j + \lambda}
  \]

  - $G_j = \sum_{i \in I_j} g_i$, $H_j = \sum_{i \in I_j} h_i$

#### **f. Update Predictions**

- Update $\hat{y}_i^{(t)}$:

  $  \hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + w_{q(x_i)}
  \]

### **Step 4: Model Output**

- After all iterations, the final model is:

  $  \hat{y}_i = \sum_{t=1}^T f_t(x_i)
  \]

---

**Example of Calculations**

Suppose we have the following data for a regression problem:

- $y_i$: Target values.
- $\hat{y}_i^{(t-1)}$: Current predictions.

Compute gradients and Hessians:

- $g_i = \hat{y}_i^{(t-1)} - y_i$
- $h_i = 1$

Proceed to build the tree using the steps outlined.

---

**Handling Missing Values**

XGBoost can automatically learn the best direction to handle missing values during split finding.

- For each split, it computes gains considering missing values going to either left or right.
- Chooses the direction with the higher gain.

---

**Regularization and Overfitting**

- **$\gamma$**: Penalizes the number of leaves (complexity of the tree).
- **$\lambda$**: L2 regularization on leaf weights.

These parameters help prevent overfitting by controlling the complexity of the model.

---

**Advantages of XGBoost**

- **Speed and Performance**: Optimized for efficiency.
- **Regularization**: Reduces overfitting.
- **Parallelization**: Utilizes multiple cores.
- **Handling of Missing Data**: Robust to missing values.
- **Feature Importance**: Provides insights into the model.

---

**Conclusion**

XGBoost is a powerful algorithm that builds upon the principles of gradient boosting with enhancements in speed, performance, and handling of overfitting. By following the steps outlined in this guide, one can implement XGBoost for regression and classification tasks with a deep understanding of the underlying mathematical concepts.
